{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "settled-ozone",
   "metadata": {},
   "source": [
    "Creating a CLIP-like model (Contrastive Language–Image Pretraining) with multi-head attention in TensorFlow involves building two separate encoders—one for images and one for text—and then aligning their representations using contrastive loss.\n",
    "\n",
    "Here’s a simplified version of a CLIP-style model using:\n",
    "\n",
    "A vision encoder (with Conv and MHA)\n",
    "A text encoder (with Embedding + MHA)\n",
    "Contrastive loss to bring matching image-text pairs closer in embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# ==== Vision Encoder ====\n",
    "def build_vision_encoder(image_shape=(224, 224, 3), projection_dim=256):\n",
    "    inputs = layers.Input(shape=image_shape)\n",
    "    \n",
    "    x = layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(projection_dim)(x)\n",
    "    outputs = layers.LayerNormalization()(x)\n",
    "    \n",
    "    return Model(inputs, outputs, name=\"vision_encoder\")\n",
    "\n",
    "# ==== Text Encoder ====\n",
    "def build_text_encoder(vocab_size=10000, max_len=40, projection_dim=256):\n",
    "    inputs = layers.Input(shape=(max_len,))\n",
    "    x = layers.Embedding(vocab_size, projection_dim)(inputs)\n",
    "    \n",
    "    # Multi-head self-attention\n",
    "    attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=projection_dim)(x, x)\n",
    "    x = layers.Add()([x, attention_output])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(projection_dim)(x)\n",
    "    outputs = layers.LayerNormalization()(x)\n",
    "    \n",
    "    return Model(inputs, outputs, name=\"text_encoder\")\n",
    "\n",
    "# ==== CLIP Head ====\n",
    "class CLIPModel(tf.keras.Model):\n",
    "    def __init__(self, image_shape=(224, 224, 3), vocab_size=10000, max_len=40, projection_dim=256):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        self.vision_encoder = build_vision_encoder(image_shape, projection_dim)\n",
    "        self.text_encoder = build_text_encoder(vocab_size, max_len, projection_dim)\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super(CLIPModel, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def compute_loss(self, image_embeddings, text_embeddings):\n",
    "        # Normalize embeddings\n",
    "        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n",
    "        text_embeddings = tf.math.l2_normalize(text_embeddings, axis=1)\n",
    "\n",
    "        # Cosine similarity matrix\n",
    "        logits = tf.matmul(image_embeddings, text_embeddings, transpose_b=True)\n",
    "        # logits[i][j] is the cosine similarity (dot product of normalized vectors) between: image i and text j\n",
    "        # So the full logits matrix contains all pairwise similarities between every image-text pair in the batch.\n",
    "        labels = tf.range(tf.shape(logits)[0])\n",
    "\n",
    "        # Cross-entropy loss in both directions\n",
    "        loss_i2t = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_t2i = tf.keras.losses.sparse_categorical_crossentropy(labels, tf.transpose(logits), from_logits=True)\n",
    "        return (tf.reduce_mean(loss_i2t) + tf.reduce_mean(loss_t2i)) / 2\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, texts = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            image_embeddings = self.vision_encoder(images, training=True)\n",
    "            text_embeddings = self.text_encoder(texts, training=True)\n",
    "            loss = self.compute_loss(image_embeddings, text_embeddings)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIPModel()\n",
    "clip.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Dummy data\n",
    "import numpy as np\n",
    "batch_size = 8\n",
    "dummy_images = np.random.rand(batch_size, 224, 224, 3).astype(\"float32\")\n",
    "dummy_texts = np.random.randint(0, 10000, (batch_size, 40))\n",
    "\n",
    "# Train step\n",
    "clip.train_on_batch((dummy_images, dummy_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-operator",
   "metadata": {},
   "source": [
    "Notes\n",
    "This is simplified: the actual CLIP uses Vision Transformers and BPE tokenization with larger models.\n",
    "You can upgrade the vision encoder to a ViT and use Transformer blocks for text for a closer replica.\n",
    "Multi-head attention allows the model to focus on different aspects of the input simultaneously.\n",
    "Want to extend it with Transformer blocks, cosine similarity logits scaling, or pretrained backbones like ResNet/BERT? Let me know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
